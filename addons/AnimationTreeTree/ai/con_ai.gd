# spinalcord
# https://github.com/spinalcord/animation-tree-tree-godot
# 
# This file is part of animation-tree-tree-godot.
# 
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public.
#
# NOTE: The output generated by this addon is not affected by this license.

# ConAI.gd
class_name ConAI
extends RefCounted

enum Role
{
	System,
	User,
	Assistant,
	Tool
}
var http_request: HTTPRequest

var model: String = "local-model"
var temperature: float = 0.6
var max_tokens: int = 100000
var base_url: String = "http://localhost:1234"
## Prevents "Context Overflow" with "Sliding Window Algorithm". Affects only the API Message.
var context_window_slide_size: int = 15
var api_key: String = ""  # Add this line
var feedback: FeedbackDialog = FeedbackDialog.new()

func _init(container: DependencyContainer):
	# Load config values or use defaults
	model = AnimationTreeTree.plugin_config.get_value("settings", "model", "local-model")
	temperature = AnimationTreeTree.plugin_config.get_value("settings", "temperature", 0.6)
	max_tokens = AnimationTreeTree.plugin_config.get_value("settings", "max_tokens", 60000)
	base_url = AnimationTreeTree.plugin_config.get_value("settings", "base_url", "http://localhost:1234")
	context_window_slide_size = AnimationTreeTree.plugin_config.get_value("settings", "context_window_slide_size", 15)
	api_key = AnimationTreeTree.plugin_config.get_value("settings", "api_key", "") 

	http_request = HTTPRequest.new()
	if Engine.is_editor_hint():
		EditorInterface.get_base_control().add_child(http_request)

## Emitted when a tool is called with its name and arguments
signal tool_called(tool_name: String, arguments: Dictionary)
## Emitted when a tool execution is completed with its result
signal tool_result(tool_name: String, result: String)
## Emitted when the entire message process completed successfully
signal message_completed(conversation: Conversation)
## Emitted when any error occurs that prevents getting a final answer
signal message_error()
## Emitted when the dialog is force-closed by the user
signal dialog_cancelled()

var _is_cancelled: bool = false

# Main function to get an AI answer using Agent and Conversation
func answer(agent: Agent, conversation: Conversation, tools: ConAITool, message: String, debug_title: String = "") -> void:
	_is_cancelled = false
	TreeDebug.msg(debug_title + "Request üôã: " + message)
	# Backup conversation state for rollback
	var backup_chain = conversation.chain.duplicate(true)
	var backup_timestamp_last = conversation.timestamp_last
	
	# Add user message to conversation
	conversation.add_message(message, Role.find_key(Role.User).to_lower())
	
	# Build message array: system prompt + conversation chain
	var msg_array: Array = []
	msg_array.append({"role": "system", "content": agent.system_prompt})
	msg_array.append_array(conversation.chain)
	
	# Apply sliding window if needed
	var truncated_messages = truncate_sliding_window(msg_array, context_window_slide_size)
	await _process_message(tools, truncated_messages, conversation, backup_chain, backup_timestamp_last)

## Internal function to process messages
func _process_message(tools: ConAITool, msg_array: Array, conversation: Conversation, backup_chain: Array, backup_timestamp_last: float) -> void:
	# Show progress dialog
	var progress_dialog = feedback.show_progress("Sending request to AI...", "Processing", false)
	
	var dialog_closed_by_user = false
	var on_dialog_hide = func():
		dialog_closed_by_user = true
		_is_cancelled = true
		dialog_cancelled.emit()
		TreeDebug.msg("‚ö†Ô∏è Dialog closed by user")
	
	if progress_dialog.has_signal("popup_hide"):
		progress_dialog.connect("popup_hide", on_dialog_hide)
	elif progress_dialog.has_signal("close_requested"):
		progress_dialog.connect("close_requested", on_dialog_hide)
	elif progress_dialog.has_signal("visibility_changed"):
		var on_visibility = func():
			if not progress_dialog.visible:
				on_dialog_hide.call()
		progress_dialog.connect("visibility_changed", on_visibility)
	
	var headers = ["Content-Type: application/json"]
	if api_key != "":
		headers.append("Authorization: Bearer " + api_key)
		TreeDebug.msg("üîë Using API Key authentication")
	else:
		TreeDebug.msg("‚ö†Ô∏è No API Key set - using local endpoint")
		
	var body = {
		"model": model,
		"messages": msg_array,
		"temperature": temperature,
		"max_tokens": max_tokens
	}
	
	# Add tools only if provided
	if tools != null:
		body["tools"] = tools.get_structure()
		TreeDebug.msg(str(body["tools"]))
	# Smart endpoint building
	var endpoint_url = base_url
	if not endpoint_url.ends_with("/completions"):
		if endpoint_url.contains("/openai"):
			endpoint_url += "/chat/completions"
		else:
			endpoint_url += "/v1/chat/completions"
	
	TreeDebug.msg("üì° Endpoint: " + endpoint_url)
	TreeDebug.msg("üì§ Sending request with model: " + model)
	
	http_request.request(endpoint_url, headers, HTTPClient.METHOD_POST, JSON.stringify(body))
	feedback.update_progress(progress_dialog, "Waiting for response...")
	
	var response = await http_request.request_completed
	
	if _is_cancelled or not is_instance_valid(progress_dialog) or not progress_dialog.visible:
		_is_cancelled = true
		if is_instance_valid(progress_dialog):
			feedback.close_progress(progress_dialog)
		conversation.chain = backup_chain
		conversation.timestamp_last = backup_timestamp_last
		TreeDebug.msg("‚ùå Request cancelled by user")
		return
	
	var http_result = response[0]  # result code
	var response_code = response[1]  # HTTP response code
	var response_headers = response[2]  # headers
	var body_data = response[3]  # body
	
	TreeDebug.msg("üì• HTTP Result: " + str(http_result) + " | Response Code: " + str(response_code) )
	
	if http_result == HTTPRequest.Result.RESULT_SUCCESS and response_code == 200:
		feedback.update_progress(progress_dialog, "Processing response...")
		
		var json = JSON.new()
		json.parse(body_data.get_string_from_utf8())
		var jsonObj: Dictionary = json.data
		var choice = jsonObj["choices"][0]
		
		if choice.has("finish_reason") and choice["finish_reason"] == "length":
			feedback.close_progress(progress_dialog)
			# Rollback conversation on error
			conversation.chain = backup_chain
			conversation.timestamp_last = backup_timestamp_last
			message_error.emit()
			assert(false, "Response was truncated by the endpoint! Increase max_tokens.")
			return
			
		choice = choice.get("message", {})
		
		var content: String = ""
		if choice is Dictionary and choice.has("content") and choice["content"] != null:
			content = str(choice["content"])
		
		var has_tools_field: bool = choice is Dictionary and choice.has("tool_calls") and choice["tool_calls"] != null

		
		var tool_calls: Array = []
		if has_tools_field:
			tool_calls = choice["tool_calls"]
		
		var need_tool_request: bool = tool_calls.size() > 0 and tools != null
		
		if need_tool_request:
			if _is_cancelled or not is_instance_valid(progress_dialog) or not progress_dialog.visible:
				_is_cancelled = true
				if is_instance_valid(progress_dialog):
					feedback.close_progress(progress_dialog)
				conversation.chain = backup_chain
				conversation.timestamp_last = backup_timestamp_last
				TreeDebug.msg("‚ùå Request cancelled by user")
				return
			
			feedback.update_progress(progress_dialog, "Executing tools...")
			
			# Add the assistant message with tool calls to the messages
			var assistant_message = {
				"role": "assistant",
				"tool_calls": tool_calls
			}
			msg_array.append(assistant_message)
			conversation.chain.append(assistant_message)
			
			# Execute all tool calls
			for tool_call in tool_calls:
				if _is_cancelled or not is_instance_valid(progress_dialog) or not progress_dialog.visible:
					_is_cancelled = true
					if is_instance_valid(progress_dialog):
						feedback.close_progress(progress_dialog)
					conversation.chain = backup_chain
					conversation.timestamp_last = backup_timestamp_last
					TreeDebug.msg("‚ùå Request cancelled by user")
					return
				
				var tool_id = tool_call["id"]
				var function_name = tool_call["function"]["name"]
				var arguments_str = tool_call["function"]["arguments"]
				var arguments_dict = JSON.parse_string(arguments_str)
				
				TreeDebug.msg("Use Tool üîß: " + function_name)
				feedback.update_progress(progress_dialog, "Using tool: " + function_name)
				
				tool_called.emit(function_name, arguments_dict)
				
				# Execute the tool
				var result = await tools.call_tool(function_name, arguments_dict)
				
				tool_result.emit(function_name, result)
				
				# Add tool result to messages
				var tool_message = {
					"role": "tool",
					"content": str(result),
					"tool_call_id": tool_id
				}
				msg_array.append(tool_message)
				conversation.chain.append(tool_message)
			
			if _is_cancelled or not is_instance_valid(progress_dialog) or not progress_dialog.visible:
				_is_cancelled = true
				if is_instance_valid(progress_dialog):
					feedback.close_progress(progress_dialog)
				conversation.chain = backup_chain
				conversation.timestamp_last = backup_timestamp_last
				TreeDebug.msg("‚ùå Request cancelled by user")
				return
			
			# Multi-step tool calling loop
			var max_tool_iterations = 5
			var current_iteration = 0
			var continue_loop = true
			
			while continue_loop and current_iteration < max_tool_iterations:
				current_iteration += 1
				feedback.update_progress(progress_dialog, "Getting response (step " + str(current_iteration) + ")...")
				
				var tool_body = {
					"model": model,
					"messages": msg_array,
					"temperature": temperature,
					"max_tokens": max_tokens
				}
				
				# Add tools to allow further tool calls
				if tools != null:
					tool_body["tools"] = tools.get_structure()
				
				http_request.request(endpoint_url, headers, HTTPClient.METHOD_POST, JSON.stringify(tool_body))
				var tool_response = await http_request.request_completed
				
				if _is_cancelled or not is_instance_valid(progress_dialog) or not progress_dialog.visible:
					_is_cancelled = true
					if is_instance_valid(progress_dialog):
						feedback.close_progress(progress_dialog)
					conversation.chain = backup_chain
					conversation.timestamp_last = backup_timestamp_last
					TreeDebug.msg("‚ùå Request cancelled by user")
					return
				
				var tool_http_result = tool_response[0]
				var tool_response_code = tool_response[1]
				var tool_response_headers = tool_response[2]
				var tool_body_data = tool_response[3]
				
				if tool_http_result == HTTPRequest.Result.RESULT_SUCCESS and tool_response_code == 200:
					var tool_json = JSON.new()
					tool_json.parse(tool_body_data.get_string_from_utf8())
					var tool_jsonObj: Dictionary = tool_json.data
					var tool_choice = tool_jsonObj["choices"][0]
					
					if tool_choice.has("finish_reason") and tool_choice["finish_reason"] == "length":
						feedback.close_progress(progress_dialog)
						conversation.chain = backup_chain
						conversation.timestamp_last = backup_timestamp_last
						message_error.emit()
						assert(false, "Tool response was truncated! Increase max_tokens.")
						return
					
					tool_choice = tool_choice.get("message", {})
					
					# Check if there are more tool calls
					var has_more_tools = tool_choice is Dictionary and tool_choice.has("tool_calls") and tool_choice["tool_calls"] != null
					var next_tool_calls: Array = []
					if has_more_tools:
						next_tool_calls = tool_choice["tool_calls"]
					
					if next_tool_calls.size() > 0:
						print("üîÑ AI requested " + str(next_tool_calls.size()) + " more tools (iteration " + str(current_iteration) + ")")
						
						# Add assistant message with tool calls
						var next_assistant_message = {
							"role": "assistant",
							"tool_calls": next_tool_calls
						}
						msg_array.append(next_assistant_message)
						conversation.chain.append(next_assistant_message)
						
						# Execute all tool calls
						for tool_call in next_tool_calls:
							if _is_cancelled or not is_instance_valid(progress_dialog) or not progress_dialog.visible:
								_is_cancelled = true
								if is_instance_valid(progress_dialog):
									feedback.close_progress(progress_dialog)
								conversation.chain = backup_chain
								conversation.timestamp_last = backup_timestamp_last
								TreeDebug.msg("‚ùå Request cancelled by user")
								return
							
							var tool_id = tool_call["id"]
							var function_name = tool_call["function"]["name"]
							var arguments_str = tool_call["function"]["arguments"]
							var arguments_dict = JSON.parse_string(arguments_str)
							
							TreeDebug.msg("Use Tool üîß: " + function_name + " (iteration " + str(current_iteration) + ")")
							feedback.update_progress(progress_dialog, "Using tool: " + function_name)
							
							tool_called.emit(function_name, arguments_dict)
							var result = await tools.call_tool(function_name, arguments_dict)
							tool_result.emit(function_name, result)
							
							# Add tool result to messages
							var tool_message = {
								"role": "tool",
								"content": str(result),
								"tool_call_id": tool_id
							}
							msg_array.append(tool_message)
							conversation.chain.append(tool_message)
						
						# Continue loop for next iteration
						continue_loop = true
					else:
						# No more tool calls, extract final content
						var final_content: String = ""
						if tool_choice is Dictionary and tool_choice.has("content") and tool_choice["content"] != null:
							final_content = str(tool_choice["content"])
						
						# Add the final answer to conversation
						conversation.add_message(final_content, "assistant")
						
						feedback.close_progress(progress_dialog)
						TreeDebug.msg("Response with Multi-Step Tool-Usage ü§ñ (" + str(current_iteration) + " iterations): " + final_content)
						message_completed.emit(conversation)
						
						# Exit loop
						continue_loop = false
				else:
					feedback.close_progress(progress_dialog)
					conversation.chain = backup_chain
					conversation.timestamp_last = backup_timestamp_last
					TreeDebug.msg("Tool request failed. Status: ", tool_response_code)
					message_error.emit()
					return
			
			# Check if we hit max iterations
			if current_iteration >= max_tool_iterations:
				TreeDebug.msg("‚ö†Ô∏è Reached max tool iterations (" + str(max_tool_iterations) + ")")
		else:
			# Normal answer without tools
			conversation.add_message(content, "assistant")
			# Support for saving file, need that in other version iterations
			# save_conversation(conversation, "user://conversations/chat.json")
			
			feedback.close_progress(progress_dialog)
			message_completed.emit(conversation)
			
	else:
		# Important if we save our file to json.
		feedback.close_progress(progress_dialog)
		# Rollback conversation on error
		conversation.chain = backup_chain
		conversation.timestamp_last = backup_timestamp_last
		
		var failed_string: String = "‚ùå‚ùå‚ùå ERROR: Request failed! ‚ùå‚ùå‚ùå\n"
		failed_string += "Response Code:" + str(http_result) + "\n"
		failed_string += "HTTP Result===========\n" + str(http_result) + "\n"
		failed_string += "Raw Response==========\n" + body_data.get_string_from_utf8() + "\n"
		
		var error_json = JSON.new()
		var parse_result = error_json.parse(body_data.get_string_from_utf8())
		if parse_result == OK and error_json.data != null:
			failed_string += "JSON Error==========\n" + str(error_json) + "\n"
		else:
			failed_string += "\nCould not parse error response\n"
			
		if !(error_json.data != null):
			failed_string += "POST===========\n" + "POST response was null, is the server running?" + "\n"
			
		feedback.show_text("Request failed!", "Request failed!", failed_string, true)
		
		message_error.emit()


###############################
## loading and saving
###############################
func save_conversation(conversation: Conversation, file_path: String) -> bool:
	if not conversation or conversation.chain.is_empty():
		push_warning("No conversation to save")
		return false
	
	# Erstelle Verzeichnis falls es nicht existiert
	var dir_path = file_path.get_base_dir()
	if not DirAccess.dir_exists_absolute(dir_path):
		DirAccess.open("user://").make_dir_recursive(dir_path.trim_prefix("user://"))
	
	# Speichere Conversation-Zustand
	var save_data = {
		"chain": conversation.chain,
		"timestamp_started": conversation.timestamp_started,
		"timestamp_last": conversation.timestamp_last
	}
	
	var file = FileAccess.open(file_path, FileAccess.WRITE)
	if not file:
		push_error("Could not open file for writing: " + file_path)
		return false
	
	var json_string = JSON.stringify(save_data, "\t")
	file.store_string(json_string)
	file.close()
	
	TreeDebug.msg("Conversation saved to: " + file_path + " (" + str(conversation.chain.size()) + " messages)")
	return true

func load_conversation(file_path: String) -> Conversation:
	if not FileAccess.file_exists(file_path):
		push_warning("Conversation file does not exist: " + file_path)
		return null
	
	var file = FileAccess.open(file_path, FileAccess.READ)
	if not file:
		push_error("Could not open file for reading: " + file_path)
		return null
	
	var json_string = file.get_as_text()
	file.close()
	
	var json = JSON.new()
	var parse_result = json.parse(json_string)
	
	if parse_result != OK:
		push_error("Failed to parse JSON from file: " + file_path)
		return null
	
	var data = json.data
	if not data is Dictionary:
		push_error("Invalid conversation format in file: " + file_path)
		return null
	
	var conversation = Conversation.new()
	conversation.chain = data.get("chain", [])
	conversation.timestamp_started = data.get("timestamp_started", 0.0)
	conversation.timestamp_last = data.get("timestamp_last", 0.0)
	
	TreeDebug.msg("Conversation loaded from: " + file_path + " (" + str(conversation.chain.size()) + " messages)")
	return conversation

###############################
## messages optimization
###############################

## Reduces the message array with sliding window algorithm.
func _on_dialog_cancelled() -> void:
	_is_cancelled = true
	dialog_cancelled.emit()
	TreeDebug.msg("‚ö†Ô∏è Dialog cancelled by user")

func truncate_sliding_window(target_messages: Array, window_size: int) -> Array:
	if target_messages.is_empty() or (window_size <= 0):
		return target_messages
	
	var result = []
	var system_role = Role.find_key(Role.System).to_lower()
	var user_role = Role.find_key(Role.User).to_lower()
	var assistant_role = Role.find_key(Role.Assistant).to_lower()
	
	var system_message = null
	for message in target_messages:
		if message.get("role", "") == system_role:
			system_message = message
			break
	
	if system_message:
		result.append(system_message)
	
	var first_user_message = null
	for message in target_messages:
		if message.get("role", "") == user_role:
			first_user_message = message
			break
	
	if first_user_message:
		result.append(first_user_message)
	
	var needs_truncation = target_messages.size() > (3 + window_size)
	
	var truncation_notice = null
	if needs_truncation:
		truncation_notice = {
			"role": assistant_role,
			"content": "[Conversation history truncated to maintain context window]"
		}
		result.append(truncation_notice)
	
	var non_system_messages = []
	for message in target_messages:
		if message.get("role", "") != system_role:
			non_system_messages.append(message)
	
	if first_user_message and non_system_messages.size() > 0 and non_system_messages[0] == first_user_message:
		non_system_messages = non_system_messages.slice(1)
	
	var recent_messages = non_system_messages.slice(-window_size) if non_system_messages.size() > window_size else non_system_messages
	
	while not recent_messages.is_empty() and recent_messages[-1].get("role", "") == assistant_role:
		recent_messages = recent_messages.slice(0, -1)
	
	result.append_array(recent_messages)
	
	if not result.is_empty():
		var final_message = result[-1]
		var final_role = final_message.get("role", "")
		if final_role == assistant_role and (not truncation_notice or final_message != truncation_notice):
			push_warning("Warning: Conversation flow ends with Assistant message - this may cause issues")
		else:
			var action = "retained" if not needs_truncation else "truncated successfully"
			TreeDebug.msg("Conversation " + action + ": " + str(result.size()) + " messages")
	
	return result


	
	
